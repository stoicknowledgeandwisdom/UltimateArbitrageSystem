global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@arbitrage-system.com'
  smtp_auth_username: 'alerts@arbitrage-system.com'
  smtp_auth_password: '{{SMTP_PASSWORD}}'
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'
  slack_api_url: '{{SLACK_WEBHOOK_URL}}'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 5s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  routes:
    # Critical alerts go to PagerDuty immediately
    - match:
        severity: critical
      receiver: 'pagerduty-critical'
      group_wait: 0s
      repeat_interval: 5m
      continue: true
    
    # Trading system alerts
    - match:
        service: trading
      receiver: 'trading-team'
      group_wait: 10s
      repeat_interval: 30m
      continue: true
    
    # Risk management alerts - highest priority
    - match:
        service: risk
      receiver: 'risk-team-urgent'
      group_wait: 0s
      repeat_interval: 2m
      continue: true
    
    # SLO breaches
    - match:
        service: slo
      receiver: 'sre-team'
      group_wait: 1m
      repeat_interval: 1h
    
    # Security alerts
    - match_re:
        alertname: '.*Security.*|.*Intrusion.*|.*Anomaly.*'
      receiver: 'security-team'
      group_wait: 5s
      repeat_interval: 15m
    
    # System alerts
    - match:
        service: system
      receiver: 'infrastructure-team'
      group_wait: 2m
      repeat_interval: 2h

receivers:
  - name: 'default'
    slack_configs:
      - api_url: '{{SLACK_WEBHOOK_URL}}'
        channel: '#alerts-general'
        title: 'Arbitrage System Alert'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Severity:* {{ .Labels.severity }}
          *Service:* {{ .Labels.service }}
          {{ end }}
        send_resolved: true

  - name: 'pagerduty-critical'
    pagerduty_configs:
      - routing_key: '{{PAGERDUTY_ROUTING_KEY}}'
        description: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary }}'
        severity: 'critical'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          service: '{{ .GroupLabels.service }}'
        links:
          - href: 'http://grafana:3000/d/trading-dashboard'
            text: 'Trading Dashboard'
          - href: 'http://prometheus:9090/alerts'
            text: 'Prometheus Alerts'

  - name: 'trading-team'
    slack_configs:
      - api_url: '{{SLACK_WEBHOOK_URL}}'
        channel: '#trading-alerts'
        title: 'üö® Trading System Alert'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Severity:* {{ .Labels.severity }}
          *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
        send_resolved: true
        actions:
          - type: 'button'
            text: 'View Dashboard'
            url: 'http://grafana:3000/d/trading-dashboard'
          - type: 'button'
            text: 'Acknowledge'
            url: 'http://alertmanager:9093/api/v1/silences'

  - name: 'risk-team-urgent'
    pagerduty_configs:
      - routing_key: '{{PAGERDUTY_RISK_ROUTING_KEY}}'
        description: 'üî¥ RISK ALERT: {{ .CommonAnnotations.summary }}'
        severity: 'critical'
        class: 'risk-management'
        component: 'trading-risk'
    slack_configs:
      - api_url: '{{SLACK_WEBHOOK_URL}}'
        channel: '#risk-alerts'
        title: 'üî¥ URGENT RISK ALERT'
        text: |
          @channel IMMEDIATE ATTENTION REQUIRED
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
        send_resolved: true

  - name: 'sre-team'
    slack_configs:
      - api_url: '{{SLACK_WEBHOOK_URL}}'
        channel: '#sre-alerts'
        title: 'üìä SLO Alert'
        text: |
          {{ range .Alerts }}
          *SLO Breach:* {{ .Annotations.summary }}
          *Details:* {{ .Annotations.description }}
          *SLO Type:* {{ .Labels.slo_type }}
          {{ if .Labels.burn_rate }}*Burn Rate:* {{ .Labels.burn_rate }}{{ end }}
          {{ end }}
        send_resolved: true

  - name: 'security-team'
    slack_configs:
      - api_url: '{{SLACK_WEBHOOK_URL}}'
        channel: '#security-alerts'
        title: 'üõ°Ô∏è Security Alert'
        text: |
          {{ range .Alerts }}
          *Security Event:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Severity:* {{ .Labels.severity }}
          *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}
        send_resolved: true
    email_configs:
      - to: 'security@arbitrage-system.com'
        subject: 'üõ°Ô∏è Security Alert: {{ .GroupLabels.alertname }}'
        body: |
          Security alert triggered in the arbitrage system:
          
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Severity: {{ .Labels.severity }}
          Time: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}

  - name: 'infrastructure-team'
    slack_configs:
      - api_url: '{{SLACK_WEBHOOK_URL}}'
        channel: '#infrastructure-alerts'
        title: 'üñ•Ô∏è Infrastructure Alert'
        text: |
          {{ range .Alerts }}
          *Infrastructure Issue:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Instance:* {{ .Labels.instance }}
          *Severity:* {{ .Labels.severity }}
          {{ end }}
        send_resolved: true

# Inhibition rules to reduce noise
inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']
  
  - source_match:
      alertname: 'ExchangeConnectivityLoss'
    target_match_re:
      alertname: '.*Trading.*|.*Arbitrage.*'
    equal: ['instance']

# Auto-silence rules during maintenance windows
silences:
  - matchers:
      - name: 'maintenance'
        value: 'true'
    starts_at: '2024-01-01T00:00:00Z'
    ends_at: '2024-12-31T23:59:59Z'
    created_by: 'system'
    comment: 'Maintenance window silencing'

