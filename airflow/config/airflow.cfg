[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where your airflow DAGs are stored
dags_folder = /opt/airflow/dags

# The folder where airflow stores its log files
base_log_folder = /opt/airflow/logs

# The executor class that airflow should use
executor = KubernetesExecutor

# The SqlAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load the examples that ship with Airflow
load_examples = False

# Whether to expose the configuration file in the web server
expose_config = False

# Time zone
default_timezone = UTC

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using
base_url = http://localhost:8080

# Default DAG view
dag_default_view = graph

# The amount of time (in secs) webserver will wait for initial handshake
web_server_worker_timeout = 120

# Number of workers to refresh at a time
worker_refresh_batch_size = 1

# Number of workers to refresh at a time
worker_refresh_interval = 30

# Secret key for Flask sessions
secret_key = your-secret-key-here

[scheduler]
# Task instances listen for external kill signal (when you `airflow tasks clear` them)
catchup_by_default = False

# The scheduler constantly tries to trigger new tasks
scheduler_heartbeat_sec = 5

# The number of times to retry a task
max_tis_per_query = 512

# Statsd (https://github.com/etsy/statsd) integration settings
statsd_on = False

[kubernetes]
# The repository, tag and imagePullPolicy of the Kubernetes Image for the Worker to Run
worker_container_repository = apache/airflow
worker_container_tag = 2.7.0
worker_container_image_pull_policy = Always

# The Kubernetes namespace where airflow workers should be created
namespace = airflow

# The Key-value pairs to be given to worker pods
worker_pods_creation_batch_size = 1

# Resource specifications for the worker pods
worker_container_memory_request = 512Mi
worker_container_memory_limit = 1024Mi
worker_container_cpu_request = 100m
worker_container_cpu_limit = 1000m

# The worker pod's PersistentVolume
volume_claim_template = {
    "kind": "PersistentVolumeClaim",
    "apiVersion": "v1",
    "metadata": {
        "name": "airflow-worker-pvc"
    },
    "spec": {
        "accessModes": ["ReadWriteOnce"],
        "resources": {
            "requests": {
                "storage": "10Gi"
            }
        }
    }
}

# Environment variables for worker pods
worker_container_env = {
    "PYTHONPATH": "/opt/airflow",
    "AIRFLOW__CORE__DAGS_FOLDER": "/opt/airflow/dags",
    "AIRFLOW__CORE__EXECUTOR": "KubernetesExecutor"
}

[api]
# How to authenticate users of the API
auth_backend = airflow.api.auth.backend.basic_auth

[operators]
# The default owner assigned to each new operator
default_owner = airflow

# Default mapreduce queue for HiveOperator tasks
default_queue = default

[smtp]
# SMTP configuration
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 587
smtp_mail_from = airflow@example.com

[celery]
# This section is ignored when using KubernetesExecutor
result_backend = db+postgresql://airflow:airflow@postgres:5432/airflow

[logging]
# Logging level
logging_level = INFO

# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Remote logging
remote_logging = False

[metrics]
# StatsD Metrics
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

